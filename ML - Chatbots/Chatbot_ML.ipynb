{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtFT1mWZxmdy",
        "outputId": "e3471366-e4e0-4333-d9f0-239bffcf9ea9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "\n",
        "# Get sequence length max of 1024\n",
        "tokenizer.model_max_length=1024\n",
        "\n",
        "model.eval()  # disable dropout (or leave in train mode to finetune)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjb7p_b1z6h9",
        "outputId": "9a1f4fa1-5d82-4b41-d7af-9843035e4ccb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:1833: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input sequence\n",
        "text = \"o que é fortnite? fortnite é um\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# model output\n",
        "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "loss, logits = outputs[:2]\n",
        "predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
        "predicted_text = tokenizer.decode([predicted_index])\n",
        "\n",
        "# results\n",
        "print('input text:', text)\n",
        "print('predicted text:', predicted_text)\n",
        "\n",
        "# input text: Quem era Jim Henson? Jim Henson era um\n",
        "# predicted text:  homem\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbsHXYcWC1Xj",
        "outputId": "6b97efae-90ae-4e66-ca42-6a2f06071dd6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input text: o que é fortnite? fortnite é um\n",
            "predicted text:  termo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelWithLMHead\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "model = TFAutoModelWithLMHead.from_pretrained(\"pierreguillou/gpt2-small-portuguese\")\n",
        "\n",
        "# Get sequence length max of 1024\n",
        "tokenizer.model_max_length=1024\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg7YIp2nDH9G",
        "outputId": "d4381de4-bf22-4a4e-e816-1f4fe9b99139"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_tf_auto.py:721: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at pierreguillou/gpt2-small-portuguese.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input sequence\n",
        "text = \"o que é fortnite? fortnite é um\"\n",
        "inputs = tokenizer.encode(text, return_tensors=\"tf\")\n",
        "\n",
        "# model output using Top-k sampling text generation method\n",
        "outputs = model.generate(inputs, eos_token_id=50256, pad_token_id=50256,\n",
        "                         do_sample=True,\n",
        "                         max_length=40,\n",
        "                         top_k=40)\n",
        "print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "# >> Generated text\n",
        "# Quem era Jim Henson? Jim Henson era um amigo familiar da família. Ele foi contratado pelo seu pai\n",
        "# para trabalhar como aprendiz no escritório de um escritório de impressão, e então começou a ganhar dinheiro\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb_G3KcmDktV",
        "outputId": "ca77284e-fed2-4f13-c6ae-e2c19b804efe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o que é fortnite? fortnite é um termo que aplica-se a qualquer tipo de classificação aplicada a uma espécie de determinada característica, como por exemplo, a qualidade das florestas ou de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xNZMWLlDoew",
        "outputId": "10018f00-d637-4831-8ff6-ac2b0dac6660"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator"
      ],
      "metadata": {
        "id": "loOBw1gkLxci"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def traduzir_palavra(palavra, idioma_origem='en', idioma_destino='pt'):\n",
        "  \"\"\"\n",
        "  Traduz uma palavra de um idioma para outro usando o Google Translate.\n",
        "\n",
        "  Args:\n",
        "    palavra: A palavra a ser traduzida.\n",
        "    idioma_origem: O idioma de origem da palavra (padrão: 'en' para inglês).\n",
        "    idioma_destino: O idioma de destino da tradução (padrão: 'pt' para português).\n",
        "\n",
        "  Returns:\n",
        "    A tradução da palavra ou None em caso de erro.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    tradutor = Translator()\n",
        "    traducao = tradutor.translate(palavra, src=idioma_origem, dest=idioma_destino)\n",
        "    return traducao.text\n",
        "  except Exception as e:\n",
        "    print(f\"Erro ao traduzir: {e}\")\n",
        "    return None\n",
        "\n",
        "# Exemplo de uso:\n",
        "palavra_ingles = \"watermelon\"\n",
        "palavra_portugues = traduzir_palavra(palavra_ingles)\n",
        "\n",
        "if palavra_portugues:\n",
        "  print(f\"'{palavra_ingles}' em português é '{palavra_portugues}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7jdXEt7MCOB",
        "outputId": "658e0eaa-df22-4fd6-944b-d502aeb6db78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'watermelon' em português é 'melancia'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUs9R3oZckHs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}